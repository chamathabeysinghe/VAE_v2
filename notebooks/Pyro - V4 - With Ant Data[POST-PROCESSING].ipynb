{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import visdom\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.examples.multi_mnist as multi_mnist\n",
    "import pyro.optim as optim\n",
    "import pyro.poutine as poutine\n",
    "from components.AIR import AIR, latents_to_tensor\n",
    "from pyro.contrib.examples.util import get_data_directory\n",
    "from pyro.infer import SVI, JitTraceGraph_ELBO, TraceGraph_ELBO\n",
    "from utils.viz import draw_many, tensor_to_objs\n",
    "from utils.visualizer import plot_mnist_sample\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['-v', '--verbose'], dest='verbose', nargs=0, const=True, default=False, type=None, choices=None, help='write hyper parameters and network architecture to stdout', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Pyro AIR example\", argument_default=argparse.SUPPRESS)\n",
    "parser.add_argument('-n', '--num-steps', type=int, default=int(1e8),\n",
    "                    help='number of optimization steps to take')\n",
    "parser.add_argument('-b', '--batch-size', type=int, default=64,\n",
    "                    help='batch size')\n",
    "parser.add_argument('-lr', '--learning-rate', type=float, default=1e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('-blr', '--baseline-learning-rate', type=float, default=1e-3,\n",
    "                    help='baseline learning rate')\n",
    "parser.add_argument('--progress-every', type=int, default=1,\n",
    "                    help='number of steps between writing progress to stdout')\n",
    "parser.add_argument('--eval-every', type=int, default=0,\n",
    "                    help='number of steps between evaluations')\n",
    "parser.add_argument('--baseline-scalar', type=float,\n",
    "                    help='scale the output of the baseline nets by this value')\n",
    "parser.add_argument('--no-baselines', action='store_true', default=False,\n",
    "                    help='do not use data dependent baselines')\n",
    "parser.add_argument('--encoder-net', type=int, nargs='+', default=[200],\n",
    "                    help='encoder net hidden layer sizes')\n",
    "parser.add_argument('--decoder-net', type=int, nargs='+', default=[200],\n",
    "                    help='decoder net hidden layer sizes')\n",
    "parser.add_argument('--predict-net', type=int, nargs='+',\n",
    "                    help='predict net hidden layer sizes')\n",
    "parser.add_argument('--embed-net', type=int, nargs='+',\n",
    "                    help='embed net architecture')\n",
    "parser.add_argument('--bl-predict-net', type=int, nargs='+',\n",
    "                    help='baseline predict net hidden layer sizes')\n",
    "parser.add_argument('--non-linearity', type=str,\n",
    "                    help='non linearity to use throughout')\n",
    "parser.add_argument('--viz', action='store_true', default=True,\n",
    "                    help='generate vizualizations during optimization')\n",
    "parser.add_argument('--viz-every', type=int, default=100,\n",
    "                    help='number of steps between vizualizations')\n",
    "parser.add_argument('--visdom-env', default='main',\n",
    "                    help='visdom enviroment name')\n",
    "parser.add_argument('--load', type=str, default=\"/Users/chamathabeysinghe/Projects/monash/VAE_v2/checkpoints/model-size-75-3ants.ckpt\",\n",
    "                    help='load previously saved parameters')\n",
    "parser.add_argument('--save', type=str, default=\"/Users/chamathabeysinghe/Projects/monash/VAE_v2/checkpoints/model-size-75-3ants.ckpt\",\n",
    "                    help='save parameters to specified file')\n",
    "parser.add_argument('--save-every', type=int, default=100,\n",
    "                    help='number of steps between parameter saves')\n",
    "parser.add_argument('--cuda', action='store_true', default=False,\n",
    "                    help='use cuda')\n",
    "parser.add_argument('--jit', action='store_true', default=False,\n",
    "                    help='use PyTorch jit')\n",
    "parser.add_argument('-t', '--model-steps', type=int, default=3,\n",
    "                    help='number of time steps')\n",
    "parser.add_argument('--rnn-hidden-size', type=int, default=256,\n",
    "                    help='rnn hidden size')\n",
    "parser.add_argument('--encoder-latent-size', type=int, default=50,\n",
    "                    help='attention window encoder/decoder latent space size')\n",
    "parser.add_argument('--decoder-output-bias', type=float,\n",
    "                    help='bias added to decoder output (prior to applying non-linearity)')\n",
    "parser.add_argument('--decoder-output-use-sigmoid', action='store_true',\n",
    "                    help='apply sigmoid function to output of decoder network')\n",
    "parser.add_argument('--window-size', type=int, default=28,\n",
    "                    help='attention window size')\n",
    "parser.add_argument('--z-pres-prior', type=float, default=0.5,\n",
    "                    help='prior success probability for z_pres')\n",
    "parser.add_argument('--z-pres-prior-raw', action='store_true', default=False,\n",
    "                    help='use --z-pres-prior directly as success prob instead of a geometric like prior')\n",
    "parser.add_argument('--anneal-prior', choices='none lin exp'.split(), default='none',\n",
    "                    help='anneal z_pres prior during optimization')\n",
    "parser.add_argument('--anneal-prior-to', type=float, default=1e-7,\n",
    "                    help='target z_pres prior prob')\n",
    "parser.add_argument('--anneal-prior-begin', type=int, default=0,\n",
    "                    help='number of steps to wait before beginning to anneal the prior')\n",
    "parser.add_argument('--anneal-prior-duration', type=int, default=100000,\n",
    "                    help='number of steps over which to anneal the prior')\n",
    "parser.add_argument('--pos-prior-mean', type=float,\n",
    "                    help='mean of the window position prior')\n",
    "parser.add_argument('--pos-prior-sd', type=float,\n",
    "                    help='std. dev. of the window position prior')\n",
    "parser.add_argument('--scale-prior-mean', type=float,\n",
    "                    help='mean of the window scale prior')\n",
    "parser.add_argument('--scale-prior-sd', type=float,\n",
    "                    help='std. dev. of the window scale prior')\n",
    "parser.add_argument('--no-masking', action='store_true', default=False,\n",
    "                    help='do not mask out the costs of unused choices')\n",
    "parser.add_argument('--seed', type=int, help='random seed', default=None)\n",
    "parser.add_argument('-v', '--verbose', action='store_true', default=False,\n",
    "                    help='write hyper parameters and network architecture to stdout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vars(parser.parse_args(\"\"))\n",
    "args = argparse.Namespace(**vars(parser.parse_args(\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prior(k):\n",
    "    assert 0 < k <= 1\n",
    "    u = 1 / (1 + k + k**2 + k**3)\n",
    "    p0 = 1 - u\n",
    "    p1 = 1 - (k * u) / p0\n",
    "    p2 = 1 - (k**2 * u) / (p0 * p1)\n",
    "    trial_probs = [p0, p1, p2]\n",
    "    # dist = [1 - p0, p0 * (1 - p1), p0 * p1 * (1 - p2), p0 * p1 * p2]\n",
    "    # print(dist)\n",
    "    return lambda t: trial_probs[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'save' in args:\n",
    "#     if os.path.exists(args.save):\n",
    "#         raise RuntimeError('Output file \"{}\" already exists.'.format(args.save))\n",
    "\n",
    "if args.seed is not None:\n",
    "    pyro.set_rng_seed(args.seed)\n",
    "\n",
    "# Build a function to compute z_pres prior probabilities.\n",
    "if args.z_pres_prior_raw:\n",
    "    def base_z_pres_prior_p(t):\n",
    "        return args.z_pres_prior\n",
    "else:\n",
    "    base_z_pres_prior_p = make_prior(args.z_pres_prior)\n",
    "\n",
    "# Wrap with logic to apply any annealing.\n",
    "def z_pres_prior_p(opt_step, time_step):\n",
    "    p = base_z_pres_prior_p(time_step)\n",
    "    if args.anneal_prior == 'none':\n",
    "        return p\n",
    "    else:\n",
    "        decay = dict(lin=lin_decay, exp=exp_decay)[args.anneal_prior]\n",
    "        return decay(p, args.anneal_prior_to, args.anneal_prior_begin,\n",
    "                     args.anneal_prior_duration, opt_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(base_path):\n",
    "    path = base_path + '/{:05d}.png'\n",
    "    X_np = []\n",
    "    for i in range(SAMPLE_COUNT):      \n",
    "        img = np.asarray(Image.open(path.format(i)))\n",
    "        X_np.append(img)\n",
    "        \n",
    "    X_np = np.asarray(X_np)    \n",
    "    X_np = X_np.astype(np.float32)\n",
    "    X_np /= 255.0\n",
    "    X = torch.from_numpy(X_np)\n",
    "    # Using FloatTensor to allow comparison with values sampled from\n",
    "    # Bernoulli.\n",
    "    counts = torch.FloatTensor([1 for objs in X_np])\n",
    "    return X, counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arg_keys = ['window_size',\n",
    "                  'rnn_hidden_size',\n",
    "                  'decoder_output_bias',\n",
    "                  'decoder_output_use_sigmoid',\n",
    "                  'baseline_scalar',\n",
    "                  'encoder_net',\n",
    "                  'decoder_net',\n",
    "                  'predict_net',\n",
    "                  'embed_net',\n",
    "                  'bl_predict_net',\n",
    "                  'non_linearity',\n",
    "                  'pos_prior_mean',\n",
    "                  'pos_prior_sd',\n",
    "                  'scale_prior_mean',\n",
    "                  'scale_prior_sd']\n",
    "model_args = {key: getattr(args, key) for key in model_arg_keys if key in args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air = AIR(\n",
    "        num_steps=args.model_steps,\n",
    "        x_size=75,\n",
    "        use_masking=not args.no_masking,\n",
    "        use_baselines=not args.no_baselines,\n",
    "        z_what_size=args.encoder_latent_size,\n",
    "        use_cuda=args.cuda,\n",
    "        **model_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'load' in args:\n",
    "    print('Loading parameters...')\n",
    "    air.load_state_dict(torch.load(args.load))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 100\n",
    "vis = visdom.Visdom(env=args.visdom_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2arr(img):\n",
    "    # assumes color image\n",
    "    # returns an array suitable for sending to visdom\n",
    "#     return np.asarray(img)\n",
    "    return np.array(img.getdata(), np.uint8).reshape(img.size + (3,)).transpose((2, 0, 1))\n",
    "\n",
    "def arr2img(arr):\n",
    "    # arr is expected to be a 2d array of floats in [0,1]\n",
    "    return Image.frombuffer('L', arr.shape, (arr * 255).astype(np.uint8).tostring(), 'raw', 'L', 0, 1)\n",
    "\n",
    "def bounding_box(z_where, x_size):\n",
    "    \"\"\"This doesn't take into account interpolation, but it's close\n",
    "    enough to be usable.\"\"\"\n",
    "    w = x_size / z_where.s\n",
    "    h = x_size / z_where.s\n",
    "    xtrans = -z_where.x / z_where.s * x_size / 2.\n",
    "    ytrans = -z_where.y / z_where.s * x_size / 2.\n",
    "    x = (x_size - w) / 2 + xtrans  # origin is top left\n",
    "    y = (x_size - h) / 2 + ytrans\n",
    "    return (x, y), w, h\n",
    "\n",
    "def colors(k):\n",
    "    return [(255, 0, 0), (0, 255, 0), (0, 0, 255)][k % 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_one(imgarr, z_arr):\n",
    "    # Note that this clipping makes the visualisation somewhat\n",
    "    # misleading, as it incorrectly suggests objects occlude one\n",
    "    # another.\n",
    "    clipped = np.clip(imgarr.detach().cpu().numpy(), 0, 1)\n",
    "    img = arr2img(clipped).convert('RGB')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for k, z in enumerate(z_arr):\n",
    "        # It would be better to use z_pres to change the opacity of\n",
    "        # the bounding boxes, but I couldn't make that work with PIL.\n",
    "        # Instead this darkens the color, and skips boxes altogether\n",
    "        # when z_pres==0.\n",
    "        if z.pres > 0:\n",
    "            (x, y), w, h = bounding_box(z, imgarr.size(0))\n",
    "            color = tuple(map(lambda c: int(c * z.pres), colors(k)))\n",
    "            crop_img = clipped[int(y):int(y)+int(h), int(x):int(x)+int(w)]\n",
    "            white_count = np.count_nonzero(crop_img>0.01)\n",
    "            black_count = np.count_nonzero(crop_img<0.01)\n",
    "            if (black_count > 0 and white_count / black_count < 0.09):\n",
    "                continue\n",
    "            draw.rectangle([x, y, x + w, y + h], outline=color)\n",
    "    is_relaxed = any(z.pres != math.floor(z.pres) for z in z_arr)\n",
    "    fmtstr = '{:.1f}' if is_relaxed else '{:.0f}'\n",
    "    draw.text((0, 0), fmtstr.format(sum(z.pres for z in z_arr)), fill='white')\n",
    "    return img2arr(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_many_custom(imgarrs, z_arr):\n",
    "    # canvases is expected to be a (n,w,h) numpy array\n",
    "    # z_where_arr is expected to be a list of length n\n",
    "    return [draw_one(imgarr, z) for (imgarr, z) in zip(imgarrs.cpu(), z_arr)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_viz = X[0:50]\n",
    "trace = poutine.trace(air.guide).get_trace(examples_to_viz, None)\n",
    "z, recons = poutine.replay(air.prior, trace=trace)(examples_to_viz.size(0))\n",
    "z_wheres = tensor_to_objs(latents_to_tensor(z))\n",
    "\n",
    "# Show data with inferred objection positions.\n",
    "vis.images(draw_many_custom(examples_to_viz, z_wheres))\n",
    "# Show reconstructions of data.\n",
    "vis.images(draw_many(examples_to_viz, z_wheres))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes_img(img, bboxes):\n",
    "    img = Image.fromarray((img*255).astype(np.uint8), mode='L').convert('RGB')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    color = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        x, y, w, h = bbox\n",
    "        draw.rectangle([x, y, x + w, y + h], outline=color[i%3])\n",
    "    return np.asarray(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_bw_ratio(img, bboxes, box_stds):\n",
    "    selected_bboxes = []\n",
    "    selected_stds = []\n",
    "    selected_bw_ratios = []\n",
    "    for i in range(len(bboxes)):\n",
    "        bbox = bboxes[i]\n",
    "        x, y, w, h = bbox\n",
    "        clipped = np.clip(img, 0, 1)\n",
    "        crop_img = clipped[int(y):int(y)+int(h), int(x):int(x)+int(w)]\n",
    "        white_count = np.count_nonzero(crop_img>0.01)\n",
    "        black_count = np.count_nonzero(crop_img<0.01)\n",
    "        if (black_count > 0 and white_count / black_count < 0.09):\n",
    "            continue\n",
    "        selected_bboxes.append(bboxes[i])\n",
    "        selected_stds.append(box_stds[i])\n",
    "        selected_bw_ratios.append(white_count / max(black_count, .00001))\n",
    "    return selected_bboxes, selected_stds, selected_bw_ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes_for_image(img, z_arr):\n",
    "    bounding_boxes = []\n",
    "    for k, z in enumerate(z_arr):\n",
    "        if z.pres > 0:\n",
    "            (x, y), w, h = bounding_box(z, img.shape[0])\n",
    "            x, y, w, h = x.item(), y.item(), w.item(), h.item()\n",
    "            bounding_boxes.append([x, y, w, h])\n",
    "    return bounding_boxes\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_predict(img):\n",
    "    img_array = np.asarray([np.copy(img) for _ in range(REPEAT_COUNT)])\n",
    "    img_array = torch.from_numpy(img_array)\n",
    "    \n",
    "    trace = poutine.trace(air.guide).get_trace(img_array, None)\n",
    "    z, recons = poutine.replay(air.prior, trace=trace)(img_array.size(0))\n",
    "    z_wheres = tensor_to_objs(latents_to_tensor(z))\n",
    "    bboxes_frame =  []\n",
    "    for counter, z in enumerate(z_wheres):\n",
    "        bboxes = get_bounding_boxes_for_image(img, z)\n",
    "        img2 = draw_bboxes_img(img, bboxes)\n",
    "        bboxes_frame.append(bboxes)\n",
    "#         plt.imshow(img2)\n",
    "#         plt.show()\n",
    "    return bboxes_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_boxes(bboxes_frame):\n",
    "    parents = [[-1, -1, -1] for _ in range(REPEAT_COUNT)]\n",
    "    original_boxes = [] # [{parent: [], childs: [[]]}]\n",
    "\n",
    "    for frame_index in range(0, REPEAT_COUNT):\n",
    "        current_boxes = bboxes_frame[frame_index]\n",
    "\n",
    "        for box_index in range(len(current_boxes)):\n",
    "            if (parents[frame_index][box_index] == -1):\n",
    "                original_boxes.append({'parent': current_boxes[box_index], 'childs': []})\n",
    "\n",
    "        if (frame_index + 1 >= REPEAT_COUNT):\n",
    "            break\n",
    "\n",
    "        future_boxes = bboxes_frame[frame_index + 1]\n",
    "\n",
    "        roi_f_box_orginal_box = [[0 for _ in range(len(original_boxes))] for _ in range(len(future_boxes))]\n",
    "        for f_box_index in range(len(future_boxes)):\n",
    "            for o_box_index in range(len(original_boxes)):\n",
    "                bb_f = future_boxes[f_box_index]\n",
    "                bb_o = original_boxes[o_box_index]['parent']\n",
    "\n",
    "                roi_f_box_orginal_box[f_box_index][o_box_index] = get_iou(\n",
    "                    {'x1': bb_f[0] ,'x2': bb_f[0] + bb_f[2] ,'y1': bb_f[1] ,'y2': bb_f[1]+bb_f[3]},\n",
    "                    {'x1': bb_o[0] ,'x2': bb_o[0] + bb_o[2] ,'y1': bb_o[1] ,'y2': bb_o[1]+bb_o[3]}\n",
    "                )\n",
    "\n",
    "    #     for f_box_index in range(len(future_boxes)):\n",
    "    #         print(roi_f_box_orginal_box[f_box_index])\n",
    "        roi_f_box_orginal_box = np.asarray(roi_f_box_orginal_box) * -1\n",
    "        row_ind, col_ind = linear_sum_assignment(roi_f_box_orginal_box)\n",
    "\n",
    "        for i in range(len(row_ind)):\n",
    "            f_f_index = row_ind[i]\n",
    "            o_f_index = col_ind[i]\n",
    "            parents[frame_index+1][f_f_index] = o_f_index\n",
    "            original_boxes[o_f_index]['childs'].append(future_boxes[f_f_index])\n",
    "        \n",
    "        return original_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(original_boxes):\n",
    "    box_means = []\n",
    "    box_stds = []\n",
    "    for i, item in enumerate(original_boxes):\n",
    "        parent = item['parent']\n",
    "        boxes = item['childs']\n",
    "        boxes.append(parent)\n",
    "        boxes = np.asarray(boxes)\n",
    "        box_means.append(np.mean(boxes, axis=0))\n",
    "        box_stds.append(np.std(boxes, axis=0))\n",
    "    return box_means, box_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_std(box_means, box_stds):\n",
    "    selected_boxes = []\n",
    "    selected_stds = []\n",
    "    for i in range(len(box_means)):\n",
    "        #Todo improve this statement\n",
    "        selected = True\n",
    "        for j in range(4):\n",
    "            if (box_stds[i][j] >= STD_THRESHOLD):\n",
    "                selected = False\n",
    "                break\n",
    "\n",
    "        if selected:\n",
    "            selected_boxes.append(box_means[i])\n",
    "            selected_stds.append(box_stds[i])\n",
    "\n",
    "    return selected_boxes, selected_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bb1 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x1, y1) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "    bb2 : dict\n",
    "        Keys: {'x1', 'x2', 'y1', 'y2'}\n",
    "        The (x, y) position is at the top left corner,\n",
    "        the (x2, y2) position is at the bottom right corner\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        in [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    assert bb1['x1'] <= bb1['x2']\n",
    "    assert bb1['y1'] <= bb1['y2']\n",
    "    assert bb2['x1'] <= bb2['x2']\n",
    "    assert bb2['y1'] <= bb2['y2']\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0    \n",
    "    \n",
    "    \n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box.\n",
    "    # NOTE: We MUST ALWAYS add +1 to calculate area when working in\n",
    "    # screen coordinates, since 0,0 is the top left pixel, and w-1,h-1\n",
    "    # is the bottom right pixel. If we DON'T add +1, the result is wrong.\n",
    "    intersection_area = (x_right - x_left + 1) * (y_bottom - y_top + 1)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1['x2'] - bb1['x1'] + 1) * (bb1['y2'] - bb1['y1'] + 1)\n",
    "    bb2_area = (bb2['x2'] - bb2['x1'] + 1) * (bb2['y2'] - bb2['y1'] + 1)\n",
    "    \n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bboxes(img):\n",
    "    bboxes_frame = multiple_predict(img)\n",
    "    original_boxes = match_boxes(bboxes_frame)\n",
    "    box_means, box_stds = calculate_mean_std(original_boxes)\n",
    "    box_means, box_stds = filter_by_std(box_means, box_stds)\n",
    "    \n",
    "    img2 = draw_bboxes_img(img, box_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    images = []\n",
    "    images_gt = []\n",
    "    csv_predictions = []\n",
    "    for i in range(SAMPLE_COUNT):\n",
    "        img = examples_to_viz.detach().cpu().numpy()[i]\n",
    "        bboxes_frame = multiple_predict(img)\n",
    "        original_boxes = match_boxes(bboxes_frame)\n",
    "        selected_boxes, box_stds = calculate_mean_std(original_boxes)\n",
    "        selected_boxes, box_stds, selected_bw_ratios = filter_by_bw_ratio(img, selected_boxes, box_stds)\n",
    "#         selected_boxes, box_stds = filter_by_std(selected_boxes, box_stds)\n",
    "        img2 = draw_bboxes_img(img, selected_boxes)\n",
    "        images.append(img2)\n",
    "        \n",
    "        \n",
    "        gt_boxes = Y_df.loc[Y_df['frame'] == i][['y', 'x', 'w', 'h']]\n",
    "        img2_gt = draw_bboxes_img(img, gt_boxes.values)\n",
    "        images_gt.append(img2_gt)\n",
    "        \n",
    "        predictions = list(map(lambda x: [i]+ list(x[0]) + [x[1]] , zip(selected_boxes, selected_bw_ratios)))\n",
    "        csv_predictions += predictions\n",
    "\n",
    "    vis_images = [img2arr(Image.fromarray(img)) for img in images]\n",
    "    vis.images(vis_images)\n",
    "    \n",
    "    vis_images_gt = [img2arr(Image.fromarray(img)) for img in images_gt]\n",
    "    vis.images(vis_images_gt)\n",
    "    return csv_predictions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_folder = 3\n",
    "dataset_type = 'simple_dataset'\n",
    "base_path = \"/Users/chamathabeysinghe/Projects/monash/VAE_v2/data/synthetic/{}/original:_300-resize:_75-{}_ants\".format(dataset_type, ant_folder)\n",
    "SAMPLE_COUNT = 100\n",
    "\n",
    "\n",
    "X, counts = load_data(base_path)\n",
    "X_size = X.size(0)\n",
    "if args.cuda:\n",
    "    X = X.cuda()\n",
    "    \n",
    "data_csv = base_path + \".csv\"\n",
    "Y_df = pd.read_csv(data_csv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 75, 75])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT_COUNT = 50\n",
    "BOX_COUNT = 3\n",
    "STD_THRESHOLD = 2\n",
    "examples_to_viz = X[0:SAMPLE_COUNT]\n",
    "\n",
    "predictions = run_test()\n",
    "\n",
    "\n",
    "predictions_df = pd.DataFrame(np.asarray(predictions), columns=['frame', 'y', 'x', 'w', 'h', 'score'])\n",
    "predictions_df = predictions_df[['frame', 'x', 'y', 'h', 'w', 'score']]\n",
    "file_name = base_path.split('/')[-1]\n",
    "save_path = '/Users/chamathabeysinghe/Projects/monash/VAE_v2/predictions/{}/{}.csv'.format(dataset_type, file_name)\n",
    "predictions_df.to_csv(save_path, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "venv_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
